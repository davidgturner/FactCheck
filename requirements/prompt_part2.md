We are working on a textual entailment project.  The dataset and overall setup for this project is taken from FActScore (Min et al., 2023).1 FActScore investigates the problem of detecting errors in biographies of people generated by LLM. The FActScore paper explores methods to decompose claims into facts, then searches over Wikipedia to find articles that (potentially) support those facts. They conduct human annotation of these steps to understand the performance of systems in different parts of this pipeline (decomposition and fact-checking). 

The facts given are propositions extracted from LLM outputs. These are simple natural
language sentences. Although FActScore explores different ways of producing these facts from biographies,
we use their human-annotated facts for our analysis, to remove any potential errors in this step. These facts
are then validated against passages that are retrieved by Wikipedia. Each fact is hand-labeled by humans
with either “S” for supported, “NS” for not supported, or “IR” for irrelevant. However, we will only predict
S vs. NS to simplify the task

Passages have been retrieved using BM25, a sparse retrieval model (one not using neural
networks). The FActScore paper also discusses experiments with a dense retriever, but passages from this
model only give slightly better performance. In the provided Python code, FactExample stores the examples themselves. These consist of a fact (a string), passages (a list of dicts consisting of a title and text), and the label. Note that the passages themselves are not tokenized or preprocessed in any way.
The raw fact data is located at data/dev labeled ChatGPT.jsonl and look like this (note: this
example is not in the dataset given to you):

{"input": "Question: Tell me a bio of Lanny Flaherty.",
"output": "Lanny Flaherty is an American actor born on December 18, 1949, in
Pensacola, Florida. He has appeared in numerous films, television shows, and
theater productions throughout his career, which began in the late 1970s.
[REMOVED]
"annotations": [
{"text": "Lanny Flaherty is an American actor born on December 18, 1949, in Pensacola, Florida.",
"is-relevant": true,
"human-atomic-facts": [
{"text": "Lanny Flaherty is an American.", "label": "S"},
{"text": "Lanny Flaherty is an actor.", "label": "S"},
{"text": "Lanny Flaherty was born on December 18, 1949.", "label": "NS"} [...]

Note that the output field is what ChatGPT generated about this person, which may or may not be
correct. The human-atomic-facts are annotated facts with labels indicating whether each part of the
output is true or not, according to humans who were able to consult Wikipedia.
The retrieved passages are in passages bm25 ChatGPT humfacts.jsonl and they look like this.
Note that only a single passage was retrieved in this case, but the passages you have are generally more numerous. This is information from Wikipedia, so is likely to be more reliable than what the model generated.
{"name": "Lanny Flaherty",
"sent": "Lanny Flaherty is an American.",
"passages": [{"title": "Lanny Flaherty",
"text": "<s>Lanny Flaherty Lanny Flaherty (born July 27, 1942) is an
American actor.</s><s>Career. He has given his most memorable performances
in \"Lonesome Dove\", \"Natural Born Killers\", \"\" and \"Signs\". Flaherty
attended University of Southern Mississippi after high school. He also
had a brief role in \"Men in Black 3\", and appeared as Jack Crow in Jim
Mickles 2014 adaptation of \"Cold in July\". Other film appearances include
\"Winter People\", \"Millers Crossing\", \"Blood In Blood Out\", \"Tom and
Huck\" and \"Home Fries\" while television roles include guest appearances
on \"The Equalizer\", \"New York News\" and \"White Collar\" as well as a 2
episode stint on \"The Education of Max Bickford\" as Whammo.</s><s>Personal
life. Flaherty resides in New York City.</s>"}]}

Please developer a textual entailment method that goes beyond the surface form of the words. 
This system should be able to decide whether a fact (the “hypothesis”) is logically entailed by a source document (the “premise”). For this part, you will use a pre-trained model, specifically a DeBERTa-v3 base model. This is a pretrained instance of DeBERTa (He et al., 2020) fine-tuned on the MNLI (Williams et al., 2018), FEVER (Thorne et al., 2018), and ANLI (Nie et al., 2020). This model can take a premise-hypothesis pair and return a decision: entailment, neutral, or contradiction. Please use this information to determine supported vs. not supported for the fact.

Please develop the assumption that you cannot run on a GPU so everything should be from the perspective of optimizing to run on a CPU so you will need to calibrate the runtime on CPU only.

The EntailmentModel class includes most of the boilerplate code needed to do inference with this model.
However, we stop short of mapping logits to an actual prediction. Please decide a strategy that
makes the most sense for extracting a binary S/NS decision from a three-class entailment/neutral/contradiction
decision. You can either use the discrete entailment decision, or you can derive a decision based on a threshold of one of the entailment/neutral/contradiction probabilities (or some combination thereof).
Entailment models are typically designed to take sentences as input, not entire passages.2 Therefore, you
will have to do some kind of sentence splitting and likely some “cleaning” as well, due to noise in the data
introduced by Wikipedia. You should then loop over the sentences and systematically compare the fact to
each sentence in the passages, then take the “max” over the passages.
If your implementation is uses the entailment classifier discretely (e.g., taking the returned entailment
label), then the sentence can be considered entailed if it is entailed by any part of any of the passages. If
your implementation uses entailment scores and sets a threshold, then the score for that fact should be the
max over the entailment scores returned by running on any example in the passage. This resembles the
method presented in Laban et al. (2022).

General Tips:

Please look at:
https://github.com/shmsw25/FActScore/blob/main/factscore/atomic_facts.py

The ANLI dataset does contain paragraphs as passages. However, in our experimentation, we did not find that feeding wholepassage premises into the model worked well. You are free to experiment with it, though!

For the Tokenizer, please use the tokenizer that can with the model (DebertaV3TokenizerFast)
and treat the <s> and </s> as the separator tokens. 

