INITIAL:
We need to develop a textual entailment model project. I will give prompts in the following order:
- Background
- Technical Task
- General Tips
- Unit Testing

Before starting on the project, please listen to all of these and ask any questions that you have. 
Once hearing the whole background, and getting all questions answered then we can proceed. 

BACKGROUND:

The dataset and overall setup for this project is taken from FActScore. FActScore investigates the problem of detecting errors in biographies of people generated by LLM. The FActScore paper explores methods to decompose claims into facts, then searches over Wikipedia to find articles that potentially support those facts. They conduct human annotation of these steps to understand the performance of systems in different parts of this pipeline (decomposition and fact-checking). 

The facts given are propositions extracted from LLM outputs. These are simple natural
language sentences. Although FActScore explores different ways of producing these facts from biographies,
we use their human-annotated facts for analysis, to remove any potential errors in this step. These facts
are then validated against passages that are retrieved by Wikipedia. Each fact is hand-labeled by humans
with either “S” for supported, “NS” for not supported, or “IR” for irrelevant. However, the goal is to predict
S vs. NS to simplify the task.
Background on this can be found at the following URL locations:
- https://github.com/shmsw25/FActScore
- https://arxiv.org/pdf/2305.14251.pdf

Passages have been retrieved using BM25, a sparse retrieval model (one not using neural
networks). The FActScore paper also discusses experiments with a dense retriever, but passages from this
model only give slightly better performance. In the provided Python code, FactExample stores the examples themselves. These consist of a fact (a string), passages (a list of dicts consisting of a title and text), and the label. Note that the passages themselves are not tokenized or preprocessed in any way.
The raw fact data is located at data/dev with a labeled jsonl file and an example is provided below:

{"input": "Question: Tell me a bio of Lanny Flaherty.",
"output": "Lanny Flaherty is an American actor born on December 18, 1949, in
Pensacola, Florida. He has appeared in numerous films, television shows, and
theater productions throughout his career, which began in the late 1970s.
[REMOVED]
"annotations": [
{"text": "Lanny Flaherty is an American actor born on December 18, 1949, in Pensacola, Florida.",
"is-relevant": true,
"human-atomic-facts": [
{"text": "Lanny Flaherty is an American.", "label": "S"},
{"text": "Lanny Flaherty is an actor.", "label": "S"},
{"text": "Lanny Flaherty was born on December 18, 1949.", "label": "NS"} [...]

Note that the output field is what a LLM generated about this person, which may or may not be
correct. The human-atomic-facts are annotated facts with labels indicating whether each part of the
output is true or not, according to humans who were able to consult Wikipedia.
The retrieved passages are in passages bm25 LLM humfacts.jsonl and they look like this.
Note that only a single passage was retrieved in this case, but the passages are generally more numerous. This is information from Wikipedia, so is likely to be more reliable than what the model generated.
{"name": "Lanny Flaherty",
"sent": "Lanny Flaherty is an American.",
"passages": [{"title": "Lanny Flaherty",
"text": "<s>Lanny Flaherty Lanny Flaherty (born July 27, 1942) is an
American actor.</s><s>Career. He has given his most memorable performances
in \"Lonesome Dove\", \"Natural Born Killers\", \"\" and \"Signs\". Flaherty
attended University of Southern Mississippi after high school. He also
had a brief role in \"Men in Black 3\", and appeared as Jack Crow in Jim
Mickles 2014 adaptation of \"Cold in July\". Other film appearances include
\"Winter People\", \"Millers Crossing\", \"Blood In Blood Out\", \"Tom and
Huck\" and \"Home Fries\" while television roles include guest appearances
on \"The Equalizer\", \"New York News\" and \"White Collar\" as well as a 2
episode stint on \"The Education of Max Bickford\" as Whammo.</s><s>Personal
life. Flaherty resides in New York City.</s>"}]}

TECHNICAL TASK:
Please develop a textual entailment method that goes beyond the surface form of the words. 
This system should be able to decide whether a fact (the “hypothesis”) is logically entailed by a source document (the “premise”). This will use a pre-trained model, specifically a DeBERTa-v3 base model. This is a pretrained instance of DeBERTa (He et al., 2020) fine-tuned on the MNLI (Williams et al., 2018), FEVER (Thorne et al., 2018), and ANLI (Nie et al., 2020). This model can take a premise-hypothesis pair and return a decision: entailment, neutral, or contradiction. Please use this information to determine supported vs. not supported for the fact.

The EntailmentModel class includes most of the boilerplate code needed to do inference with this model.
However, we stop short of mapping logits to an actual prediction. Please decide a strategy that
makes the most sense for extracting a binary S/NS decision from a three-class entailment/neutral/contradiction
decision. 

Please use one or both of the following approaches:
 1.) either use the discrete entailment decision, 
    or 
 2.)  derive a decision based on a threshold of one of the entailment/neutral/contradiction probabilities (or some combination thereof).

Entailment models are typically designed to take sentences as input, not entire passages.2 Therefore, there is a need to do some kind of "sentence splitting" and likely some "cleaning" as well, due to noise in the data
introduced by Wikipedia. Then loop over the sentences and systematically compare the fact to
each sentence in the passages, then take the "max" over the passages.
If the implementation uses the entailment classifier discretely (e.g., taking the returned entailment
label), then the sentence can be considered entailed if it is entailed by any part of any of the passages. If
the implementation uses entailment scores and sets a threshold, then the score for that fact should be the
max over the entailment scores returned by running on any example in the passage. This resembles the
method presented in Laban et al. (2022).

GENERAL TIPS:
- Please look at:
https://github.com/shmsw25/FActScore/blob/main/factscore/atomic_facts.py
for tips on sentence preprocessing (cleaning and tokenization)
- The ANLI dataset does contain paragraphs as passages. However, in our experimentation, we did not find that feeding wholepassage premises into the model worked well. You are free to experiment with it, though!
- For the Tokenizer, please use the tokenizer that can with the model (DebertaV3TokenizerFast)
and treat the <s> and </s> as the separator tokens. 
- Please don't use sent_tokenize as this will not tokenize properly in accordance with the model. 
We want to use the associated tokenizer that came with the model instead (DebertaV3TokenizerFast)
- Please develop the assumption that you cannot run on a GPU so everything should be from the perspective of optimizing to run on a CPU so you will need to calibrate the runtime on CPU only.
- We can't fine-tune the model. We need to use the pre-trained model as is:
"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli"

- For the Laban reference, please refer to:
https://aclanthology.org/2022.tacl-1.10.pdf

- The general Class layout structure is like this:
import torch
from typing import List
import numpy as np
import spacy
import gc

class FactExample:

    def __init__(self, fact: str, passages: List[dict], label: str):
        self.fact = fact
        self.passages = passages
        self.label = label

    def __repr__(self):
        return repr("fact=" + repr(self.fact) + "; label=" + repr(self.label) + "; passages=" + repr(self.passages))


class EntailmentModel:
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer

    def check_entailment(self, premise: str, hypothesis: str):
        with torch.no_grad():
            inputs = self.tokenizer(premise, hypothesis, return_tensors='pt', truncation=True, padding=True)
            outputs = self.model(**inputs)
            logits = outputs.logits

        raise Exception("Not implemented")

        del inputs, outputs, logits
        gc.collect()

        # TODO return something

class FactChecker(object):

    def predict(self, fact: str, passages: List[dict]) -> str:
        raise Exception("Don't call me, call my subclasses")

class EntailmentFactChecker(object):
    def __init__(self, ent_model):
        self.ent_model = ent_model
		# TODO implement

    def predict(self, fact: str, passages: List[dict]) -> str:
		# TODO implement
        raise Exception("Implement me")

UNIT TESTS:

Please develop unit tests off the following examples. Each of these examples only have 2 passages each. 
In previous iteration attempts at this project these examples have been a challenge for the model to get 
right so we want unit tests off of these to improve and validate the approach.

Fact,Clean Fact,# of Total Passages,# of Passage with S,# of Passage with NS,Golden Label,Prediction Label,Correct?

The Fischer Research Laboratory was founded in 1936.,The Fischer Research Laboratory was founded in 1936.,2,0,2,NS,S,N

Maracaibo is in Venezuela.,Maracaibo is in Venezuela.,2,0,2,S,NS,N

Marianne McAndrew is an actress.,Marianne McAndrew is an actress.,2,0,2,S,NS,N

Rennie Fritchie is British.,Rennie Fritchie is British.,2,0,2,S,NS,N

Continuum Analytics is an organization.,Continuum Analytics is an organization.,2,0,2,S,NS,N

I will provide the passages list for each of these separately, but just assert that the predicted label 
needs to equal the golden label to pass