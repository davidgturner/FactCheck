{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "c:\\Projects\\FactCheck\\factcheck_venv\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer Algorithm: DebertaV2TokenizerFast\n",
      "Special Tokens:\n",
      "  Start Token: [CLS] ID: 1\n",
      "  End (EOS) Token: [SEP] ID: 2\n",
      "  Unknown Token: [UNK] ID: 3\n",
      "  Padding Token: [PAD] ID: 0\n",
      "  Mask Token: [MASK] ID: 128000\n",
      "Vocabulary Size: 128000\n",
      "Model Max Length: 512\n",
      "Is Lowercase: False\n"
     ]
    }
   ],
   "source": [
    "# a function to grab a single fact from json file\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from factcheck import EntailmentFactChecker, EntailmentModel \n",
    "from transformers import AutoConfig\n",
    "\n",
    "# Initialize the EntailmentFactChecker\n",
    "model_name = \"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\"\n",
    "ent_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer = ent_tokenizer\n",
    "roberta_ent_model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "ent_model = EntailmentModel(roberta_ent_model, ent_tokenizer)\n",
    "fact_checker = EntailmentFactChecker(ent_model)\n",
    "\n",
    "# Print out details about the tokenizer\n",
    "print(\"Tokenizer Algorithm:\", tokenizer.__class__.__name__)\n",
    "print(\"Special Tokens:\")\n",
    "print(\"  Start Token:\", tokenizer.bos_token, \"ID:\", tokenizer.bos_token_id)\n",
    "print(\"  End (EOS) Token:\", tokenizer.eos_token, \"ID:\", tokenizer.eos_token_id)\n",
    "print(\"  Unknown Token:\", tokenizer.unk_token, \"ID:\", tokenizer.unk_token_id)\n",
    "print(\"  Padding Token:\", tokenizer.pad_token, \"ID:\", tokenizer.pad_token_id)\n",
    "print(\"  Mask Token:\", tokenizer.mask_token, \"ID:\", tokenizer.mask_token_id)\n",
    "print(\"Vocabulary Size:\", tokenizer.vocab_size)\n",
    "print(\"Model Max Length:\", tokenizer.model_max_length)\n",
    "print(\"Is Lowercase:\", tokenizer.do_lower_case if hasattr(tokenizer, \"do_lower_case\") else \"Not Applicable\")\n",
    "\n",
    "# # If you want to see more attributes, you can print out the tokenizer's configuration\n",
    "# print(\"\\nTokenizer Configuration:\")\n",
    "# print(tokenizer.config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_fact_from_file(file_path, fact_name, threshold):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        data = [json.loads(line) for line in lines]\n",
    "        \n",
    "    # Search for the fact based on the \"sent\" field\n",
    "    fact_data = next((entry for entry in data if entry['sent'] == fact_name), None)\n",
    "    \n",
    "    if not fact_data:\n",
    "        print(f\"Error: No fact found with name '{fact_name}'.\")\n",
    "        return\n",
    "    \n",
    "    fact = fact_data['sent']\n",
    "    passages = fact_data['passages']\n",
    "    \n",
    "    result = fact_checker.check_fact_whole_passage(fact_checker.clean_text(fact), passages, threshold=threshold)\n",
    "\n",
    "    # Append the original passages and sentences to the result\n",
    "    result['original_passages'] = passages\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens for fact:  ['▁Continuum', '▁Analytics', '▁is', '▁an', '▁organization', '.']\n",
      "back to text:  Continuum Analytics is an organization.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>{\n",
       "    &quot;decision&quot;: &quot;S&quot;,\n",
       "    &quot;max_entailment_score&quot;: 0.2979828119277954,\n",
       "    &quot;max_contradiction_score&quot;: 0.0,\n",
       "    &quot;most_entailing_sentence&quot;: &quot;Travis Oliphant. feasibility of designing a high-level data-parallel language extension to Python on graphics processing units (GPUs). On January 1, 2018, Oliphant announced he was leaving Anaconda Inc. He subsequently co-founded Quansight later that same year. He is also a member of the advisory council of the non-profit scientific computing foundation NumFOCUS.&lt;/s&gt;&lt;s&gt;Books written. Oliphant is the author of the textbook \\&quot;Guide To NumPy\\&quot; and associated manuals.&lt;/s&gt;&lt;s&gt;Books written.:Articles. - - -&lt;/s&gt;&quot;,\n",
       "    &quot;most_contradicting_sentence&quot;: &quot;&quot;,\n",
       "    &quot;original_passages&quot;: [\n",
       "        {\n",
       "            &quot;title&quot;: &quot;Travis Oliphant&quot;,\n",
       "            &quot;text&quot;: &quot; feasibility of designing a high-level data-parallel language extension to Python on graphics processing units (GPUs). On January 1, 2018, Oliphant announced he was leaving Anaconda Inc. He subsequently co-founded Quansight later that same year. He is also a member of the advisory council of the non-profit scientific computing foundation NumFOCUS.&lt;/s&gt;&lt;s&gt;Books written. Oliphant is the author of the textbook \\&quot;Guide To NumPy\\&quot; and associated manuals.&lt;/s&gt;&lt;s&gt;Books written.:Articles. - - -&lt;/s&gt;&quot;\n",
       "        },\n",
       "        {\n",
       "            &quot;title&quot;: &quot;Travis Oliphant&quot;,\n",
       "            &quot;text&quot;: &quot;&lt;s&gt;Travis Oliphant Travis Oliphant is an American data scientist and businessman. He is a co-founder of NumFOCUS, 501(c)(3) nonprofit charity in the United States, and sits on its advisory board. He is also a founder of technology startup Anaconda (\\&quot;previously Continuum Analytics\\&quot;). In addition, Travis is the primary creator of NumPy and founding contributor to the SciPy packages in the Python programming language.&lt;/s&gt;&lt;s&gt;Early life and education. Oliphant has a Ph.D. in Biomedical Engineering from the Mayo Clinic and B.S. and M.S. degrees in Mathematics and Electrical Engineering from Brigham Young University.&lt;/s&gt;&lt;s&gt;Career. Oliphant was an Assistant Professor of Electrical and Computer Engineering at Brigham Young University from 2001 to 2007. In addition, he directed the BYU Biomedical Imaging Lab, and performed research on scanning impedance imaging. Oliphant served as President of Enthought from 2007 until 2011. He founded Continuum Analytics in January 2012 (subsequently renamed to Anaconda Inc.). Continuum makes the Python distribution Anaconda. In July 2015 Continuum Analytics received 24 million dollars in Series A Funding. Continuum Analytics received a $100,000 award from DARPA for the&quot;\n",
       "        }\n",
       "    ]\n",
       "}</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import html\n",
    "import json\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Custom CSS to enable word wrapping for <pre> tags\n",
    "custom_css = \"\"\"\n",
    "<style>\n",
    "    pre {\n",
    "        white-space: pre-wrap;\n",
    "    }\n",
    "</style>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(custom_css))\n",
    "\n",
    "def display_json(data):\n",
    "    formatted_json = html.escape(json.dumps(data, indent=4))\n",
    "    display(HTML(f\"<pre>{formatted_json}</pre>\"))\n",
    "\n",
    "# for interactive fact testing\n",
    "file_path = \"data/passages_bm25_ChatGPT_humfacts.jsonl\"\n",
    "fact_name = \"Continuum Analytics is an organization.\"\n",
    "tokens = ent_tokenizer.tokenize(fact_name)\n",
    "print(\"tokens for fact: \", tokens)\n",
    "\n",
    "back_to_text = ent_tokenizer.convert_tokens_to_string(tokens)\n",
    "print(\"back to text: \", back_to_text)\n",
    "\n",
    "t = 0.60\n",
    "result = test_fact_from_file(file_path, fact_name, threshold=t)\n",
    "display_json(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTING SINGLE PASSAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results=  [0.10194315016269684, 0.8856976628303528, 0.01235916931182146]\n"
     ]
    }
   ],
   "source": [
    "file_path = \"data/passages_bm25_ChatGPT_humfacts.jsonl\"\n",
    "fact_name = \"Florencia Bertotti is a singer.\"\n",
    "passage_text = \"\"\"\n",
    "<s>Florencia Bertotti Mar\\u00eda Florencia Bertotti (born 15 March 1983), better known as Florencia Bertotti is an Argentine actress, singer, songwriter and businesswoman.</s><s>Biography. Florencia Bertotti parents are Gustavo Bertotti, a jeweler and Mar\\u00eda Candelaria P\\u00e9rez Colman, a psychologist and a teacher of children with disabilities. Her parents divorced when Florencia was seven years old. She has an older sister called Clara Bertotti. Her father passed away in 1999 when she was filming the series \\\"Verano del '98\\\". Florencia studied at the Colegio Nuestra Se\\u00f1ora de la Misericordia in Recoleta, Buenos Aires, Argentina.</s><s>Personal life. On 2 December 2006 she got married in a religious ceremony with Guido Kaczka, whom she met in the recordings of \\\"Verano del '98\\\" and who was her boyfriend since then. On 10 July 2008, she gave birth to the couple's first child, a boy, whom they called Romeo Kaczka Bertottia.The couple divorced in March 2010. They both share custody of their son. Since 2010, Florencia Bertotti\n",
    "\"\"\"\n",
    "\n",
    "results_list = ent_model.check_entailment(fact_name, passage_text)\n",
    "print(\"Results= \", results_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTING CLEAN_TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_clean  Jean Daull� died in 1763.\n",
      "[6105, 41306, 436, 1675, 267, 84306, 260]\n",
      "Unknown Token  3\n"
     ]
    }
   ],
   "source": [
    "test_text = \"Jean Daull� died in 1763.\"\n",
    "text_clean = fact_checker.clean_text(test_text)\n",
    "print(\"text_clean \", text_clean)\n",
    "\n",
    "tokens = tokenizer.tokenize(test_text)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(token_ids)\n",
    "\n",
    "print(\"Unknown Token \", tokenizer.unk_token_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
